# CLAUDE.md

# Developer Notes

- Dont use comments, write readable descriptive code, with tools like type hinting.
- Use `.venv/bin/python3` to run the application


This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

Skillmaker is a documentation crawler that converts online documentation sites into consolidated markdown "skills" files. It uses Selenium to crawl documentation pages, converts them to markdown, and optionally uses Claude API to clean and deduplicate the content.

## Running the Application

```bash
# List all crawled skills with statistics
.venv/bin/python3 -m app.cli list

# Basic crawl (no LLM cleaning)
.venv/bin/python3 -m app.cli crawl <URL>

# With iterative page cleaning (LLM-generated custom functions)
.venv/bin/python3 -m app.cli crawl <URL> --clean

# Delete a crawled site
.venv/bin/python3 -m app.cli delete <URL>
```

**Available Commands:**
- `crawl <URL>` - Crawl a documentation site and save as markdown
- `crawl <URL> --clean` - Crawl with iterative LLM-based page cleaning
- `list` - List all crawled skills with statistics
- `delete <URL>` - Delete all traces of a crawled site

**Environment Requirements:**
- `ANTHROPIC_API_KEY` or `AVANTE_ANTHROPIC_API_KEY` environment variable must be set for cleaning features

## Architecture

### Data Flow

1. **Crawling** (`app/crawl.py:crawl_and_save_pages`)
   - Selenium crawls same-domain links from the base URL
   - URL fragments (#section) are stripped to avoid duplicates of single-page docs
   - Each page is saved as markdown in `intermediate/pages/<site_name>/<page_name>.md`
   - Visited URLs and queue persisted using `app/persist.py`

2. **Iterative Page Cleaning** (optional, via `--clean`)
   - **Sampling** (`app/page_sampler.py`): Samples 5% of total content proportionally from all pages (head/tail/middle)
   - **Iterative Refinement** (`app/cleaning_refiner.py`): Core engine with adaptive behavior
     - Smooth operation: Up to 3 iterations
     - Error recovery: Extends to 5 iterations if validation errors occur
     - Stop-on-regression: Tracks best quality score, stops if quality decreases
   - **LLM-Based Quality Evaluation** (`app/cleaning_evaluator.py`):
     - Claude semantically evaluates each iteration (not just metrics)
     - Returns: quality score (0-1), assessment text, issues found, improvements needed
     - Feedback included in next iteration's refinement prompt
     - Early stopping at quality >= 0.85 (excellent)
   - **Function Generation**:
     - Initial: LLM generates `clean_line()` and `clean_doc()` from samples
     - Refinement: LLM improves functions based on quality evaluation feedback
     - Validation: Code is tested before being applied
   - **Iteration Persistence** (`app/iteration_manager.py`):
     - Each iteration saved to `intermediate/iterations/<site_name>/iteration_N.json`
     - Stores: code, before/after samples, LLM evaluation, token usage, timestamp
     - Best iteration saved separately for easy access
   - **Application** (`app/page_cleaner.py`):
     - Best cleaning functions applied to all pages via `exec()`
     - Cleaned pages saved to `intermediate/clean/<site_name>/`
     - Original pages preserved in `intermediate/pages/`

3. **Concatenation** (`app/crawl.py:concatenate_markdown_files`)
   - Individual pages (cleaned if `--clean` used, otherwise original) concatenated with `---` separators
   - Final skill file saved to `skills/<site_name>.md`

### Key Design Decisions

**URL Fragment Stripping**: Many documentation sites use fragments for navigation on single-page docs. Without stripping, the same page would be crawled multiple times. See `strip_url_fragment()` in `app/crawl.py:25`.

**Adaptive Iteration Count**: System starts with 3 iterations for cost efficiency. If validation errors occur, it extends to 5 iterations to improve success rate.

**LLM-Driven Quality Assessment**: Instead of hardcoded metrics, Claude semantically evaluates the cleaning quality by comparing before/after samples. This allows for nuanced evaluation that considers context and documentation-specific needs.

**Stop on Regression**: Quality scores are tracked across iterations. If a new iteration has lower quality than a previous one, the system stops and uses the best iteration found so far.

**Iterative Cleaning with exec()**: The page cleaning feature uses `exec()` to run LLM-generated Python code. This is safe because:
1. Code is generated by Claude API, not user input
2. Code is validated (syntax check + test execution) before being applied
3. Functions are executed in an isolated namespace
4. Iterative refinement ensures functions work correctly before being stored

**Separate Iterations Folder**: All iteration data is stored in `intermediate/iterations/<site_name>/` for debugging and transparency. This allows inspection of how the cleaning functions evolved.

## File Structure

```
app/
  __init__.py                  # (deprecated, use crawl.py)
  crawl.py                     # Main crawl logic and orchestration
  cli.py                       # CLI argument parsing and delete command
  cleaning_refiner.py          # Iterative cleaning function generation engine
  cleaning_evaluator.py        # LLM-based quality evaluation
  iteration_manager.py         # Iteration data persistence (folder-based)
  page_sampler.py              # Proportional sampling of pages for analysis
  page_cleaner.py              # Apply cleaning functions to pages via exec()
  persist.py                   # Generic persistence utilities (sets, arrays, deques)
  folder_structure.py          # Directory paths and helpers
  url_to_name.py               # URL to filesystem-safe name conversion
  logging_util.py              # Standardized logging utilities
  markdown_llm.py              # (legacy full doc cleaning, not used with --clean)

intermediate/
  pages/                       # Individual crawled pages (by site)
    <site_name>/
      page1.md
      page2.md
      ...
      pages.txt                # List of visited URLs (persist.py format)
      queue.txt                # Queue of URLs to crawl (persist.py format)
  clean/                       # Cleaned page versions (if --clean used)
    <site_name>/
      page1.md                 # Cleaned version
      page2.md
      ...
      cleaning_functions.py    # Best cleaning code
  iterations/                  # Iteration history (if --clean used)
    <site_name>/
      iteration_1/             # First iteration folder
        SAMPLE_BEFORE.md       # Original sample
        SAMPLE_AFTER.md        # Cleaned sample
        cleaning_functions.py  # Generated code
        evaluation.json        # LLM evaluation
        metadata.txt           # Token counts, timestamp
      iteration_2/             # Second iteration folder
        ...
      best/                    # Best iteration folder (copy)
        SAMPLE_BEFORE.md
        SAMPLE_AFTER.md
        cleaning_functions.py
        evaluation.json
        metadata.txt

skills/                        # Final concatenated markdown files
  <site_name>.md               # Ready-to-use skill file
```

## Iteration Folder Structure

Each `iteration_N/` folder contains:

### `metadata.txt` (ordered tuple format)
```
iteration_num: 1
timestamp: 2025-11-08T12:34:56.789123
code_generation_input_tokens: 1500
code_generation_output_tokens: 800
evaluation_input_tokens: 1200
evaluation_output_tokens: 150
total_input_tokens: 2700
total_output_tokens: 950
```

### `evaluation.json` (LLM quality assessment)
```json
{
  "quality_score": 0.85,
  "assessment": "Good cleaning, removes navigation but preserves content",
  "issues_found": ["Some footers remain"],
  "improvements_needed": ["More aggressive footer removal"],
  "patterns_removed": ["navigation", "breadcrumb"],
  "content_preserved": true,
  "token_usage": {
    "input_tokens": 1200,
    "output_tokens": 150
  }
}
```

### `SAMPLE_BEFORE.md`
The original sampled markdown content before cleaning.

### `SAMPLE_AFTER.md`
The cleaned markdown content after applying the cleaning functions.

### `cleaning_functions.py`
The Python code containing `clean_line()` and `clean_doc()` functions generated for this iteration.

